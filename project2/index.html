<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 2 - CS180 Portfolio</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" integrity="sha512-mIs9kKbaw6JZFfSuo+MovJHGJDRshipwe7UWlbpCMXoPPETgOFthe7U2f0M8gKCWHzMRoMuL/se2/UPOJIm+PA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
            color: #333;
            margin: 0;
            background-image: url('../assets/images/background.jpg');
            background-attachment: fixed;
            background-size: cover;
            background-position: center;
            min-height: 100vh;
            box-sizing: border-box;
        }
        .container {
            max-width: 1000px;
            margin: 40px auto 80px;
            padding: 20px;
            background: rgba(255, 255, 255, 0.9);
            border-radius: 8px;
        }
        header {
            text-align: center;
            margin-bottom: 40px;
        }
        header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }
        header p {
            color: #6c757d;
        }
        .project-section {
            margin-bottom: 40px;
        }
        .project-section h2 {
            border-bottom: 2px solid #e9ecef;
            padding-bottom: 10px;
            margin-bottom: 20px;
        }
        /* Style for code blocks */
        pre[class*="language-"] {
            background-color: #e9ecef;
            padding: 15px;
            border-radius: 8px;
            margin: 20px 0;
        }
        .results-gallery {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            align-items: start;
        }
        .result-item img {
            width: 100%;
            border-radius: 8px;
            border: 1px solid #ddd;
        }
        .result-item figcaption {
            font-size: 0.9em;
            color: #6c757d;
            margin-top: 10px;
            text-align: center;
        }
        .single-image-container {
            text-align: center;
            margin: 20px auto;
            max-width: 50%; /* Match the width of one gallery item */
        }
        .single-image-container img {
            width: 100%;
            border-radius: 8px;
            border: 1px solid #ddd;
        }
        .three-col-gallery {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 20px;
            align-items: start;
        }
        .four-col-gallery {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr 1fr;
            gap: 20px;
            align-items: start;
        }
        footer {
            text-align: center;
            margin-top: 40px;
            color: #6c757d;
            font-size: 0.9em;
        }
        .back-link {
            display: block;
            text-align: center;
            margin-bottom: 20px;
        }
    </style>
</head>
<body>
    <div class="container">
        <a href="../index.html" class="back-link">Back to Portfolio</a>
        <header>
            <h1>Project 2: Fun with Filters and Frequencies</h1>
            <p>A brief description of the project will go here.</p>
        </header>

        <main>
            <section class="project-section">
                <h2>Part 1: Fun with Filters</h2>
                <h3>1.1: Convolution</h3>
                <p>In this section, I implemented convolutions from scratch using numpy operations.
                    I had a naive four for loops implementation, and two two for loops implementations that are more efficient.
                    I also implemented various modes of padding (how I dealt with the four edges). All are explained below.
                </p>
                
                <h4>Code</h4>
                <p>I took a modular approach for convolutions, implementing the padding function below that pads images according to some specifications (like full or valid).
                    After implementing my first two for loop implementation, I wasn't satisfied with the runtime, so I decided to try to optimize it further. All strategies implemented produced visually indistinguishable results.
                    See below for details on runtime and strategies.
                </p>
                <pre><code class="language-python">def pad(signal, kernel, mode='full'):
    kernel_height, kernel_width = kernel.shape
    if mode == 'full':
        return np.pad(signal, ((kernel_height - 1, kernel_height - 1), (kernel_width - 1, kernel_width - 1)))
    elif mode == 'same':
        if kernel_height % 2 == 0:
            pad_height_before = kernel_height // 2
            pad_height_after = kernel_height // 2 - 1
        else:
            pad_height_before, pad_height_after = kernel_height // 2, kernel_height // 2
        if kernel_width % 2 == 0:
            pad_width_before = kernel_width // 2
            pad_width_after = kernel_width // 2 - 1
        else:
            pad_width_before, pad_width_after = kernel_width // 2, kernel_width // 2
        return np.pad(signal, ((pad_height_before, pad_height_after), (pad_width_before, pad_width_after)))
    elif mode == 'valid':
        return signal
    else:
        print("Padding mode not supported")
        return</code></pre>
                <h5>4 For-Loops</h5>
                <pre><code class="language-python">def convolve_naive(signal, kernel, mode='full'):
    signal = pad(signal, kernel, mode)
    kernel = kernel[::-1, ::-1]

    kernel_height, kernel_width = kernel.shape
    signal_height, signal_width = signal.shape

    curr_res = None
    for signal_row in range(signal_height - kernel_height + 1):
        curr_row = []
        for signal_col in range(signal_width - kernel_width + 1):
            # Convolve
            curr_convolve_res = 0
            for kernel_row in range(kernel_height):
                for kernel_col in range(kernel_width):
                    signal_section = signal[signal_row + kernel_row][signal_col + kernel_col]
                    curr_convolve_res += kernel[kernel_row][kernel_col] * signal_section
            curr_row.append(curr_convolve_res)

        if curr_res is None:
            curr_res = np.array([curr_row])
        else:
            curr_res = np.vstack((curr_res, curr_row))
    
    return curr_res</code></pre>
                <p>This is a standard naive implementation of convolution, with no usage of parallelism offered by numpy. It calls pad first.
                    The outer two for loops iterate over "pixels" within the image/signal, and "anchors" the top left corner of the section of image to be multiplied with the kernel.
                    The inner two for loops iterate over positions within the kernel, performing multiplications with the corresponding positions in the image/signal.
                    <br>
                    Applying the 9x9 box filter on a picture of me, this took around <b>200 seconds</b> (I have a relatively old computer).
                </p>

                <h5>2 For-Loops (Optimization 1)</h5>
                <pre><code class="language-python">def convolve_optimized(signal, kernel, mode='full'):
    signal = pad(signal, kernel, mode)
    kernel = kernel[::-1, ::-1]

    kernel_height, kernel_width = kernel.shape
    signal_height, signal_width = signal.shape

    curr_res = None
    for signal_row in range(signal_height - kernel_height + 1):
        curr_row = []
        for signal_col in range(signal_width - kernel_width + 1):
            # Convolve
            convolve_res = np.sum(signal[signal_row:signal_row + kernel_height,
                                    signal_col:signal_col + kernel_width] * kernel)
            curr_row.append(convolve_res)

        if curr_res is None:
            curr_res = np.array([curr_row])
        else:
            curr_res = np.vstack((curr_res, curr_row))
    
    return curr_res</code></pre>
                <p>Dissatisfied with the runtime of the 4 for loops implementation, I decided to use 2 for loops.
                    Here, the outer two for loops iterate over positions in the image/signal, anchoring the top left corner for the current application of the kernel.
                    This is like the 4 for loops implementation. However, instead of doing the multiplications with the kernel individually, I used numpy, which parallelizes elementwise multiplications, to replace my inner 2 for loops.
                    <br>
                    Applying the 9x9 box filter on a picture of me, this took around <b>25 seconds</b>.
                </p>

                <h5>2 For-Loops (Optimization 2)</h5>
                <pre><code class="language-python">def convolve_optimized2(signal, kernel, mode='full'):
    signal = pad(signal, kernel, mode)
    kernel = kernel[::-1, ::-1]

    kernel_height, kernel_width = kernel.shape
    signal_height, signal_width = signal.shape

    res_height, res_width = signal_height - kernel_height + 1, signal_width - kernel_width + 1

    curr_res = np.zeros((res_height, res_width))
    for kernel_row in range(kernel_height):
        curr_row = []
        for kernel_col in range(kernel_width):
            # Convolve
            signal_section = signal[kernel_row:kernel_row + res_height, kernel_col:kernel_col + res_width]
            convolve_res = kernel[kernel_row][kernel_col] * signal_section
            curr_res += convolve_res
    
    return curr_res</code></pre>
                <p>I was still dissatisfied about the runtime of the 2 for loops optimization, so I decided to optimize more.
                    I noticed that the result of convolutions is just essentially a sum of each number inside of the kernel multiplied by (close to) the entire image, summed together.
                    Noting that my kernels were much smaller than my images, I wondered if it would be more efficient to iterate over the numbers inside the kernel instead.
                    That is what I did in this implementation: I iterate over numbers inside the kernel, deduce what numbers inside the image they would be multiplied to, and then summed the results.
                    <br>
                    Applying the 9x9 box filter on a picture of me, this took around <b>1 second</b>. scipy.signal.convolve2d also took around <b>1 second</b>.
                </p>

                <h4>Results</h4>
                <p>Below are the visual results of the convolution implementations compared with SciPy's library function. The original image is shown below.</p>
                
                <div class="single-image-container">
                    <figure class="result-item">
                        <img src="./assets/1.1/original_grayscale.jpg" alt="Original Image">
                        <figcaption>Original Image</figcaption>
                    </figure>
                </div>

                <h5>Box Filter</h5>
                <p>Effect: noticeable blurring of the image, notably my shirt.</p>
                <div class="results-gallery">
                    <figure class="result-item">
                        <img src="./assets/1.1/box_np.jpg" alt="My Box Filter Result">
                        <figcaption>My Implementation</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/1.1/box_scipy.jpg" alt="SciPy Box Filter Result">
                        <figcaption>SciPy Result</figcaption>
                    </figure>
                </div>

                <h5>Dx Filter</h5>
                <p>Effect: horizontal edge detection</p>
                <div class="results-gallery">
                    <figure class="result-item">
                        <img src="./assets/1.1/dx_np.jpg" alt="My Dx Filter Result">
                        <figcaption>My Implementation</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/1.1/dx_scipy.jpg" alt="SciPy Dx Filter Result">
                        <figcaption>SciPy Result</figcaption>
                    </figure>
                </div>

                <h5>Dy Filter</h5>
                <p>Effect: vertical edge detection.</p>
                <div class="results-gallery">
                    <figure class="result-item">
                        <img src="./assets/1.1/dy_np.jpg" alt="My Dy Filter Result">
                        <figcaption>My Implementation</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/1.1/dy_scipy.jpg" alt="SciPy Dy Filter Result">
                        <figcaption>SciPy Result</figcaption>
                    </figure>
                </div>

                <h3>1.2: Gradient and Edge Detection</h3>
                <p>In this subpart, I read in a cameraman image, detected its edges, and binarized the resulting gradient magnitude image for an image of edges.
                    I first applied the discrete partial derivative filters to the cameraman image, achieving the following results.</p>
                
                <div class="results-gallery">
                    <figure class="result-item">
                        <img src="./assets/1.2/dx_res.jpg" alt="Partial Derivative in X">
                        <figcaption>Partial Derivative in X (Dx)</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/1.2/dy_res.jpg" alt="Partial Derivative in Y">
                        <figcaption>Partial Derivative in Y (Dy)</figcaption>
                    </figure>
                </div>

                <p>I then made a gradient magnitude image from my discrete partial derivative images.
                    In particular, at each pixel, I took the square root of the sum of the squares of the same pixel's value in the two gradient magnitude images, and got the following result.
                </p>
                <div class="single-image-container">
                    <figure class="result-item">
                        <img src="./assets/1.2/raw_gradient_magnitude.jpg" alt="Gradient Magnitude">
                        <figcaption>Gradient Magnitude</figcaption>
                    </figure>
                </div>

                <p>Finally, I chose thresholds for each pixel; in particular, if the gradient magnitude at a pixel has value <b>greater than 0.33</b>, then the pixel gets a value of 1.
                Otherwise, the pixel gets a value of 0. I chose the threshold by experimenting with different values, and evaluating how much noise v.s. details are in the images. If there were too much noise, I highered the threshold; otherwise, I lowered the threshold.</p>
                <div class="single-image-container">
                    <figure class="result-item">
                        <img src="./assets/1.2/binarized.jpg" alt="Binarized Edge Image">
                        <figcaption>Binarized Edge Image</figcaption>
                    </figure>
                </div>

                <h3>1.3: Gaussian and Derivative of Gaussians (DoG)</h3>
                <p>In this section, I explored applying a Gaussian filter first before getting the edges from the cameraman image.
                    In particular, I explored both applying a Gaussian filter first, and then applying the discrete partial derivative kernels; and also applying derivative of Gaussian (DoG) kernels directly, which are the results from convolving the Gaussian filter with the discrete partial derivative kernels.
                    Visualizations shown below. I used 9x9 Gaussian kernels.
                </p>
                <div class="three-col-gallery">
                    <figure class="result-item">
                        <img src="./assets/1.3/gaussian_kernel.jpg" alt="Gaussian Kernel Visualization">
                        <figcaption>Gaussian Kernel</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/1.3/DOG_x.jpg" alt="DoG_x Visualization">
                        <figcaption>DoG Filter (X)</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/1.3/DOG_y.jpg" alt="DoG_y Visualization">
                        <figcaption>DoG Filter (Y)</figcaption>
                    </figure>
                </div>

                <p>The images below contrast the original cameraman image (right) v.s. the blurred cameraman image after applying a Gaussian filter.</p>

                <div class="results-gallery">
                    <figure class="result-item">
                        <img src="./assets/1.3/blurred.jpg" alt="Blurred Cameraman">
                        <figcaption>Blurred Cameraman</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/1.3/cameraman.png" alt="Original Cameraman">
                        <figcaption>Original Cameraman</figcaption>
                    </figure>
                </div>

                <p>I then applied a workflow similar to part 1.2 to get the binarized edge images.
                    In the left column, I applied a strategy identical to what I did in part 1.2 (discrete partial derivatives in both directions, then gradient magnitude image, then binarize by choosing a threshold) on the Gaussian blurred image.
                    In the right column, I applied the DoG filters in both directions instead of the discrete partial derivatives; otherwise, I employed the same strategy as part 1.2.
                    <br>
                    Note that the results in the two columns are <b>identical</b>. This is due to the associativity property of convolutions.
                </p>

                <div class="results-gallery">
                    <figure class="result-item">
                        <img src="./assets/1.3/dx_res.jpg" alt="Gaussian then Derivative Dx">
                        <figcaption>Gaussian then Derivative (Dx)</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/1.3/dx_res_2.jpg" alt="DoG Dx">
                        <figcaption>DoG Filter (Dx)</figcaption>
                    </figure>
                </div>
                <div class="results-gallery">
                    <figure class="result-item">
                        <img src="./assets/1.3/dy_res.jpg" alt="Gaussian then Derivative Dy">
                        <figcaption>Gaussian then Derivative (Dy)</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/1.3/dy_res_2.jpg" alt="DoG Dy">
                        <figcaption>DoG Filter (Dy)</figcaption>
                    </figure>
                </div>
                <div class="results-gallery">
                    <figure class="result-item">
                        <img src="./assets/1.3/raw_gradient_magnitude.jpg" alt="Gradient Magnitude from Gaussian">
                        <figcaption>Gradient Magnitude from Gaussian then Derivative</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/1.3/raw_gradient_magnitude_2.jpg" alt="Gradient Magnitude from DoG">
                        <figcaption>Gradient Magnitude from DoG</figcaption>
                    </figure>
                </div>
                <div class="results-gallery">
                    <figure class="result-item">
                        <img src="./assets/1.3/binarized.jpg" alt="Binarized from Gaussian">
                        <figcaption>Binarized from Gaussian then Derivative</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/1.3/binarized_2.jpg" alt="Binarized from DoG">
                        <figcaption>Binarized from DoG</figcaption>
                    </figure>
                </div>
                <p>Although the results of those two strategies (Gaussian then Derivative, Derivative of Gaussian) were identical, the result from 1.3 was not the same as the result from 1.2.
                    In particular, Gaussian filtering reduced some of the noise in the image, leaving the "big changes" in the image to be mostly the important edges.
                    It also did make the gradient magnitudes smaller.
                    Applying the same strategy as 1.2, I experimented with various values for the cutoff for binarization, decreasing it when there's too little details and increasing it when there's too much noise.
                    <br>
                    In the end, I finalized on a value of <b>0.1477</b> for my cutoff. This seems to have retained more detail than the previous results from part 1.2, while not being polluted by too much noise.
                    The numerical value of the cutoff itself, though, is considerably lower than what I chose for part 1.2, because of the aforementioned decrease in gradient magnitudes.
                </p>
            </section>

            <section class="project-section">
                <h2>Part 2: Fun with Frequencies</h2>
                <h3>2.1: Image "Sharpening"</h3>
                <p>In this section, I sharpened images by adding more of the high frequency components of those images to them.
                    To find the high frequency components, I subtracted the low frequency components (from Gaussian filtering) from the images.
                    Noting the distributivity of convolutions, I decided to collapse those operations into one convolution, with a kernel that is the impulse kernel minus the Gaussian kernel.
                    This allowed me to find the high frequency components with just one convolution.
                    After finding the high frequency components, I added α times the high frequencies to the original images. I clipped all values between 0.0 and 1.0. I applied this process on each channel separately.
                    <br>
                    For results below, unless otherwise stated, I used kernels of size 9x9.
                </p>

                <div class="three-col-gallery">
                    <figure class="result-item">
                        <img src="./assets/2.1/to_sharpen/taj.jpg" alt="Original Taj">
                        <figcaption>Original</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/2.1/sharpened_res/taj_1.jpg" alt="Taj Sharpened (Alpha=1)">
                        <figcaption>Sharpened (α=1)</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/2.1/sharpened_res/taj_2.jpg" alt="Taj Sharpened (Alpha=2)">
                        <figcaption>Sharpened (α=2)</figcaption>
                    </figure>
                </div>
                <div class="three-col-gallery">
                    <figure class="result-item">
                        <img src="./assets/2.1/sharpened_res/taj_3.jpg" alt="Taj Sharpened (Alpha=3)">
                        <figcaption>Sharpened (α=3)</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/2.1/sharpened_res/taj_5.jpg" alt="Taj Sharpened (Alpha=5)">
                        <figcaption>Sharpened (α=5)</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/2.1/sharpened_res/taj_high_freq.jpg" alt="Taj High Frequencies">
                        <figcaption>High Frequencies</figcaption>
                    </figure>
                </div>
                <p>Increasing the value of α (e.g. the degree to which high frequencies are added) increases the amount of visible high frequencies in the resulting image.
                    For example, lines in the image are more pronounced (especially in the section to our left of the center).
                    This makes the image look less blurry overall.
                    <br>
                    Overall, α=2 was my favorite. At α's that were too high, the images had so much high frequency lines that they looked "cartoonish". At α's that were too low, the images were too blurry.
                </p>

                <div class="three-col-gallery">
                    <figure class="result-item">
                        <img src="./assets/2.1/to_sharpen/books.jpg" alt="Original Books">
                        <figcaption>Original Books</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/2.1/sharpened_res/books.jpg" alt="Sharpened Books">
                        <figcaption>Sharpened Books</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/2.1/sharpened_res/books_high_freq.jpg" alt="Books High Frequencies">
                        <figcaption>High Frequencies</figcaption>
                    </figure>
                </div>
                <p>I then applied the aforementioned strategy to a blurry image of bookshelves that I've found online.
                    For this, I chose an α value of 8. The books (especially the separation between them) is much sharper after sharpening the image.
                </p>

                <div class="results-gallery">
                    <figure class="result-item">
                        <img src="./assets/2.1/evaluation/doe.jpg" alt="Original Doe">
                        <figcaption>Original</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/2.1/evaluation/doe_blurred.jpg" alt="Blurred Doe">
                        <figcaption>Blurred</figcaption>
                    </figure>
                </div>
                <div class="results-gallery">
                    <figure class="result-item">
                        <img src="./assets/2.1/evaluation/doe_resharpened.jpg" alt="Resharpened Doe">
                        <figcaption>Resharpened</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/2.1/evaluation/doe_high_freq.jpg" alt="Doe High Frequencies">
                        <figcaption>High Frequencies</figcaption>
                    </figure>
                </div>
                <p>I then applied the strategy to a Gaussian blurred version of an image of doe library, and then re-sharpened it.
                    I used Gaussian kernels of size 18x18, and an alpha value of 8.
                    Re-sharpening had significant effects, recreating the floral patterns in the ceiling that were close to being invisible in the blurred version.
                    This makes sense as the floral patterns are visible in the high frequency image.
                </p>

                <h3>2.2: Hybrid Images</h3>
                <p>[Present your three hybrid images. For one, detail the entire process including original images, Fourier transforms, filtered results, and cutoff frequency choice. For the other two, show the originals and the final hybrid.]</p>

                <h3>2.3 & 2.4: Gaussian/Laplacian Stacks and Blending</h3>
                <p>[Visualize the Gaussian and Laplacian stacks for the Orange/Apple images and recreate the blending results. Include your two additional custom blended images, ensuring one uses an irregular mask.]</p>
            </section>
        </main>

        <footer>
            <p>&copy; 2025 Tiger Zhang. <a href="mailto:enming_zhang@berkeley.edu">Contact Me</a></p>
        </footer>
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js" integrity="sha512-9khQRAUBYEJDCDVP2yw3LRUQvjJ0Pjx0EShmaQjcHa6AXiOv6qHQu9lCAIR8O+/D8FtaCoJ2c0Tf9Xo7hYH01Q==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-SkmBfuA2hqjzEVpmnMt/LINrjop3GKWqsuLSSB3e7iBmYK7JuWw4ldmmxwD9mdm2IRTTi00O2iPAQ5GUJWYlig==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
</body>
</html>
