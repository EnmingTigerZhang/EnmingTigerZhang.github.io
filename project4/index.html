<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 4 - CS180 Portfolio</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
            color: #333;
            margin: 0;
            background-image: url('../assets/images/background.jpg');
            background-attachment: fixed;
            background-size: cover;
            background-position: center;
            min-height: 100vh;
            box-sizing: border-box;
        }
        .container {
            max-width: 1000px;
            margin: 40px auto 80px;
            padding: 20px;
            background: rgba(255, 255, 255, 0.9);
            border-radius: 8px;
        }
        header {
            text-align: center;
            margin-bottom: 40px;
        }
        header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }
        header p {
            color: #6c757d;
        }
        .project-section {
            margin-bottom: 40px;
        }
        .project-section h2 {
            border-bottom: 2px solid #e9ecef;
            padding-bottom: 10px;
            margin-bottom: 20px;
        }
        .results-gallery {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            align-items: start;
        }
        .result-item img {
            width: 100%;
            border-radius: 8px;
            border: 1px solid #ddd;
        }
        .result-item figcaption {
            font-size: 0.9em;
            color: #6c757d;
            margin-top: 10px;
            text-align: center;
        }
        .single-image-container {
            text-align: center;
            margin: 20px auto;
            max-width: 75%;
        }
        .single-image-container img {
            width: 100%;
            border-radius: 8px;
            border: 1px solid #ddd;
        }
        .small-single-image-container {
            text-align: center;
            margin: 20px auto;
            max-width: 50%;
        }
        .small-single-image-container img {
            width: 100%;
            border-radius: 8px;
            border: 1px solid #ddd;
        }
        .three-col-gallery {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 20px;
            align-items: start;
        }
        footer {
            text-align: center;
            margin-top: 40px;
            color: #6c757d;
            font-size: 0.9em;
        }
        .back-link {
            display: block;
            text-align: center;
            margin-bottom: 20px;
        }
    </style>
</head>
<body>
    <div class="container">
        <a href="../index.html" class="back-link">Back to Portfolio</a>
        <header>
            <h1>Project 4: Neural Radiance Fields</h1>
            <p>In this project, I took pictures of a persimmon and generated novel views of it using NeRF.</p>
        </header>

        <main>
            <section class="project-section">
                <h2>Part 0: Camera Calibration and 3D Scanning</h2>
                <p>To start the project, I calibrated my camera (e.g. found its intrinsic matrix and distortion vector), and got a set of images to be used for training.
                    <br><br>
                    To calibrate my camera, I took ~50 pictures of a sheet of 6 ArUco tags using my phone camera (portrait), maintaining the same zoom (1x) but varying the distance and angle of the phone camera.
                    For defining world coordinates, I defined the bottom left of tag 4 as the origin, horizontal (e.g. towards tag 5) as the x-direction, vertical (e.g. towards tag 0) as the y-direction, and out of the page as the z-direction.
                    The entire sheet is defined to be at z=0.
                    I found that the widths of the tags are 0.06m, the horizontal distances are 0.03m, and the vertical distances are ~0.01572m. I calculated the position of the corners of the tags from those definitions.
                    I detected ArUco tags using cv2.aruco.ArucoDetector, and fed the object-point and image-point correspondences to cv2.calibrateCamera.
                    <br><br>
                    Once I calibrated my camera, I took ~70 pictures of a persimmon next to one ArUco tag (specifically tag 0), maintaining the same zoom (1x) and distance while varying the position of the phone camera.
                    I defined the bottom left corner of the ArUco tag to be the origin, and x-direction, y-direction, and z-direction similar to how I defined them for the calibration images.
                    I then detected the ArUco tags in all of the images and solved for the extrinsic matrices of all of the images via cv2.solvePnP.
                    <br><br>
                    See visualizations of the images I took below.
                </p>
                <div class="results-gallery">
                    <figure class="result-item">
                        <img src="./assets/0/1.jpeg" alt="Viser Screenshot 1">
                        <figcaption>Frustums Visualization 1</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/0/2.jpeg" alt="Viser Screenshot 2">
                        <figcaption>Frustums Visualization 2</figcaption>
                    </figure>
                </div>
                <div class="results-gallery">
                    <figure class="result-item">
                        <img src="./assets/0/3.jpeg" alt="Viser Screenshot 3">
                        <figcaption>Frustums Visualization 3</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/0/4.jpeg" alt="Viser Screenshot 4">
                        <figcaption>Frustums Visualization 4</figcaption>
                    </figure>
                </div>
                <p>After solving for the extrinsics of all of the images, I undistorted them via the script given in the project spec.
                    The effects were minimal as my camera didn't have much distortion to start with.
                </p>
            </section>

            <section class="project-section">
                <h2>Part 1: Fit a Neural Field to a 2D Image</h2>
                <p>
                    Before moving on to training 3D NeRFs, I first trained a 2D NeRF. Inputs are normalized pixel positions (with positional encoding), and expected outputs are simply rgb values.
                    <br><br>
                    I wrote a dataloader to support training with arbitrary batch sizes, and hyper-parameter tuned by training various models, and modeled two images using 2D NeRFs. More details below.
                </p>
                <h3>Model Architecture</h3>
                <div class="single-image-container">
                    <figure class="result-item">
                        <img src="./assets/1/2d_nerf.jpg" alt="Model Architecture">
                        <figcaption>2D NeRF Architecture, Credits: CS 180 Staff, Fall 2025</figcaption>
                    </figure>
                </div>
                <p>I used the model architecture given in the project spec, feeding the input (with positional encoding, L=20) through three units of {linear layer (each with 256 activations) then ReLU}, then through another linear layer mapping from 256 dimensions to 3 dimensions, and finally a sigmoid layer.
                    I trained for 3000 iterations (though the model achieved good performance long before that), sampling N=10000 pixels each iteration (by randomly sampling N integers between 0 and H - 1 as the row index and another N integers between 0 and W - 1 as the column index), and used the staff-recommended learning rate of 1e-2.
                    As mentioned earlier, for my final model, I set L=20 and W=256 (width of network). I used MSE loss and the Adam optimizer.
                </p>
                <h3>Training Progression</h3>
                <div class="single-image-container">
                    <figure class="result-item">
                        <img src="./assets/1/1_gallery.png" alt="Training Progression 1">
                        <figcaption>Training Progression on the Fox</figcaption>
                    </figure>
                </div>
                <div class="single-image-container">
                    <figure class="result-item">
                        <img src="./assets/1/2_gallery.png" alt="Training Progression 2">
                        <figcaption>Training Progression on My Image (Warren Hall)</figcaption>
                    </figure>
                </div>
                <p>
                    Note: for the iteration number above, "iteration: i" means the results after the ith iteration (0-indexed).
                    <br><br>
                    Overall, it seems like the model learned "color" first, then details.
                    This is conceptually similar to getting the low frequencies first, then the high frequencies, though the results at earlier iterations also have high-frequency features (like horizontal and vertical edges), likely due to the presence of positional encodings.
                </p>
                <h3>Final Result Grid</h3>
                <div class="results-gallery">
                    <figure class="result-item">
                        <img src="./assets/1/1_64_3.jpg" alt="Width 64, Freq 3">
                        <figcaption>Width (W): 64, Max Freq (L): 3</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/1/1_64_20.jpg" alt="Width 64, Freq 20">
                        <figcaption>Width (W): 64, Max Freq (L): 20</figcaption>
                    </figure>
                </div>
                <div class="results-gallery">
                    <figure class="result-item">
                        <img src="./assets/1/1_256_3.jpg" alt="Width 256, Freq 3">
                        <figcaption>Width (W): 256, Max Freq (L): 3</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/1/1_256_20.jpg" alt="Width 256, Freq 20">
                        <figcaption>Width (W): 256, Max Freq (L): 20</figcaption>
                    </figure>
                </div>
                <p>
                    I tried two model widths: 64 and 256; I also tried two maximum frequencies: 3 and 20.
                    <br><br>
                    Overall, it seems like the maximum frequency significantly impacted the performance of the model.
                    Holding the model width to be the same, the models with 20 as the maximum frequency were able to capture significantly more detail than their counterparts with 3 as the maximum frequency.
                    <br><br>
                    Model width also impacted model performance, though seemingly not to the same extent.
                    Holding the maximum frequency to be the same, the models with 256 as the width were able to capture slightly more detail than their counterparts with 64 as the width. (For example, consider the nose section of the fox from the two models with 20 as the maximum frequency).
                </p>
                <h3>PSNR Curve</h3>
                <div class="single-image-container">
                    <figure class="result-item">
                        <img src="./assets/1/2_plot.png" alt="PSNR Curve">
                        <figcaption>PSNR Curve (Warren Image, W=256, L=20)</figcaption>
                    </figure>
                </div>
                <p>Above is the PSNR curve for training the warren image, with model width being 256 and the maximum frequency paramete being 20.
                    The model learned quickly in the beginning, while making progress much slower after ~500 iterations (as its outputs were also close to the ground truth, as per earlier visualizations).
                </p>
                <h3>Final Rendered Results</h3>
                <div class="results-gallery">
                    <figure class="result-item">
                        <img src="./assets/1/1_5.jpg" alt="Fox Final Result">
                        <figcaption>Fox Final Result</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/1/2_5.jpg" alt="Warren Final Result">
                        <figcaption>Warren Final Result</figcaption>
                    </figure>
                </div>
            </section>

            <section class="project-section">
                <h2>Part 2: Fit a Neural Radiance Field from Multi-view Images</h2>
                <h3>Implementation Details</h3>
                <p>I then moved on to training 3D NeRF models, for which I have the following functions/classes. For most purposes, I used the batched versions of the following functions, though I may describe parallel operations as "for each" for clarity.</p>
                
                <h4>• transform(c2w, x_c)</h4>
                <p>
                    Transforms the point(s) in x_c in camera coordinates to point(s) in world coordinates using the extrinsic matrix (or matrices) in c2w.
                    <br><br>
                    I multiplied each matrix ni c2w with each vector in x_c.
                </p>

                <h4>• pixel_to_camera(K, uv, s)</h4>
                <p>
                    Transforms the point(s) in uv in pixel coordinates to point(s) in camera coordinates using the intrinsic matrix K and scale s.
                    <br><br>
                    I augmented the vectors in uv so that they had homogeneous coordinates (concatenate a 1 to each vector), scaled them by s, and right-multiplied them into the inverse of K.
                </p>

                <h4>• pixel_to_ray(K, c2w, uv)</h4>
                <p>
                    Transforms the point(s) in uv in pixel coordinates to rays, represented by an origin r_o and a normalized direction r_b. Uses transform and pixel_to_camera.
                    <br><br>
                    I first found the camera coordinates of each input point using pixel_to_camera, called on K, uv, and 1 as the scale (so one specific point along the line of possible points in camera coordinates that map to the same pixel coordinate).
                    Then, I converted those camera coordinate points to homogeneous coordinates, feeding them into transform along with c2w to get the world coordinates.
                    For each c2w matrix, I extracted out the translation vector in it, calling it r_o for the point.
                    For each point, I calculated the direction of r_b as world_coordinates - r_o, and normalized it to get r_b.
                </p>

                <h4>• sample_rays(images_train, c2ws_train, K, N_sample)</h4>
                <p>
                    Samples N_sample amount of rays randomly from the input (images_train), returning the rays and the ground-truth pixel values.
                    <br><br>
                    I had a function-based implementation for sampling rays. I sampled image_indices randomly from 0 to num_images - 1, row_indices randomly from 0 to image_H - 1, and col_indices randomly from 0 to image_W - 1.
                    All the indices are length-N_sample, and num_images, image_H, and image_W can all be calculated from image_train's shape.
                    For each sample, u = col_index + 0.5, v = row_index + 0.5.
                    I then called pixel_to_ray for all of the samples to get the rays, and directly indexed into images_train to get the pixel values.
                </p>

                <h4>• sample_along_rays(r_o, r_b, near, far, num_samples, perturb=True)</h4>
                <p>
                    Samples num_samples amount of points anchored at r_o in r_b's direction, between distance r_o and r_b.
                    <br><br>
                    I first create a linspace between near and far of length num_samples, and repeat it for each ray in this batch, calling this "t".
                    This is the amount along r_b that each sampled point will go.
                    Then, if perturb is set to True, I added noise to the t array, perturbing each entry between 0 and (far - near) / num_samples.
                    For each ray, I multiplied r_b by each entry in t, and added the results to r_o for the samples along that ray.
                </p>

                <h4>• flatten_and_positional_embedding(ray_samples, r_b, L_pos=10, L_dir=4)</h4>
                <p>
                    Takes in (Batch, num_samples, 3) ray_samples and (Batch, 3) r_b and returns (Batch * num_samples, 6 * L_pos + 3) position with positional encoding vector and (Batch * num_samples, 6 * L_dir + 3) direction with positional encoding vector.
                    This is a helper function I decided to write.
                    <br><br>
                    I flattened ray_samples into (Batch*num_samples, 3), calculated sine and cosine inputs for each of x, y, and z dimensions according to the formula given in lecture, took the sine and cosine of those inputs, and stacked the results together (along with the original flattened input).
                    <br>
                    For the direction with positional encoding vector, I followed a similar process as described above, except I calculated the sinusoidal results first, before expanding to (Batch * num_samples, 6 * L_dir + 3).
                </p>

                <h4>• NeRF(nn.Module)</h4>
                <div class="single-image-container">
                    <figure class="result-item">
                        <img src="./assets/2/3d_nerf.png" alt="Model Architecture">
                        <figcaption>3D NeRF Architecture, Credits: CS 180 Staff, Fall 2025</figcaption>
                    </figure>
                </div>
                <p>I wrote a class (subclass of nn.Module) for my NeRF model. It has the structure described in the image, though supporting for arbitrary hidden layer widths (not just 256) and positional encoding maximum frequencies for both the position (L_pos) and direction (L_dir) inputs.
                    <br><br>
                    On a high level, it has an embedding-calculation section taking in the 3d position (with positional encoding), the output of which will be used to predict the color (which also takes into account the viewing direction), as well as the density.
                </p>

                <h4>• volrend(sigmas, rgbs, step_size)</h4>
                <p>
                    Renders colors, according to the discrete version of the formula on the project spec.
                    <br><br>
                    For each ray, at each position, I calculated exp(-sigma*step_size) (which is roughly the probability of light going through this point) for use later.
                    For each position, its "color contribution" to the ray it belongs to is rgbs[position]*(1-exp(-sigma*step_size))*{product of exp(-sigma*step_size) for all previous points}.
                    The color of a ray is the sum of the "color contribution" of all of the sampled points on it.
                </p>

                <h4>• render_one_image(model, c2w, H, W, K, near=2.0, far=6.0, num_samples=32)</h4>
                <p>
                    Renders an entire image given a c2w matrix. Useful for inference and validation.
                    <br><br>
                    Iterates over all pixel coordinates in the image, adding 0.5 to both pixel coordinates to get the u, v values, which are fed to pixel_to_ray in batches to get rays, then fed to sample_along_rays (with perturb=False) to sample points along rays, then fed to flatten_and_positional_embedding to get inputs to the model, then fed into the model to get rgbs and sigmas, then rendered into colors with volrend.
                </p>

                <h3>Ray and Sample Visualization</h3>
                <p>Using the dataloading technique described above, I samples some rays (and ultimately points) in the lego dataset. Results are shown below.</p>
                <div class="results-gallery">
                    <figure class="result-item">
                        <img src="./assets/2/viser_side_top.png" alt="Side Top View">
                        <figcaption>Side-Top View</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/2/viser_side.png" alt="Side View">
                        <figcaption>Side View</figcaption>
                    </figure>
                </div>
                <h3>Training Progression</h3>
                <p>I trained a 3D NeRF model with implementation described above. Here is how the results on validation image 0 progressed through iterations.
                    <br>
                    Here are the choices I made:
                    <ul>
                        <li>Loss: MSE Loss</li>
                        <li>Optimizer: Adam optimizer</li>
                        <li>learning rate = 5e-4</li>
                        <li>num_samples = 64</li>
                        <li>near = 2.0</li>
                        <li>far = 6.0</li>
                        <li>batch_size = 10000</li>
                        <li>num_iters = 1205</li>
                        <li>L_pos = 10</li>
                        <li>L_dir = 4</li>
                        <li>network width = 256</li>
                    </ul>
                    For results below, Iteration i means results after the ith iteration.
                </p>
                <div class="three-col-gallery">
                    <figure class="result-item">
                        <img src="./assets/2/0.jpg" alt="Iteration 0">
                        <figcaption>Iteration 0</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/2/100.jpg" alt="Iteration 100">
                        <figcaption>Iteration 100</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/2/300.jpg" alt="Iteration 300">
                        <figcaption>Iteration 300</figcaption>
                    </figure>
                </div>
                <div class="three-col-gallery">
                    <figure class="result-item">
                        <img src="./assets/2/500.jpg" alt="Iteration 500">
                        <figcaption>Iteration 500</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/2/800.jpg" alt="Iteration 800">
                        <figcaption>Iteration 800</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/2/1200.jpg" alt="Iteration 1200">
                        <figcaption>Iteration 1200</figcaption>
                    </figure>
                </div>
                <h3>Results: PSNR and Rendering</h3>
                <p>
                    As above, here are the choices I made: I used MSE Loss, Adam optimizer, learning rate = 5e-4, num_samples = 64, near = 2.0, far = 6.0, batch_size = 10000, num_iters = 1205, L_pos = 10, L_dir = 4, network width = 256.
                    <br>
                    The validation PSNR surpassed 23 at around 1000, which accords with the staff solution.
                </p>
                <div class="results-gallery">
                    <figure class="result-item">
                        <img src="./assets/2/val_psnrs.png" alt="PSNR Curve">
                        <figcaption>PSNR Curve on Validation Set</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/2/test.gif" alt="Spherical Rendering">
                        <figcaption>Spherical Rendering of Lego Scene</figcaption>
                    </figure>
                </div>
            </section>

            <section class="project-section">
                <h2>Part 2.6: Training with Your Own Data</h2>
                <h3>Discussion</h3>
                <p>
                    Finally, I trained a 3D NeRF on the data I captured in part 0.
                    I employed a train-validation split where 90% of my images were in the training set, and the other 10% were in my validation set.
                    I downscaled my images to 214x286, as per recommended by the project spec; I also scaled down the first two rows of the intrinsic matrix by the same factor to account for this.
                    Learning from previous projects, to avoid aliasing, I blurred my image first with a Gaussian Kernel of size (11, 11) before downscaling.
                    <br><br>
                    The training loop includes most of the functions detailed in implementation details: I would sample rays (inputs) and their corresponding pixels from the training set for training.
                    The model architecture itself is described in the "NeRF(nn.Module)" section.
                    <br><br>
                    I performed some hyperparameter tuning, which started with guessing for a good pair of near and far values.
                    I used near=0.05 and far=0.55 from my own estimates of how far the persimmon was from the camera; those numbers worked well, so I kept them.
                    For the rest of the design choices and hyperparameters, I did not deviate far from the default and/or recommended values, and here is a summary:
                    <br>
                    <ul>
                    <li>
                    Loss: MSE Loss
                    <li>
                    Optimizer: Adam optimizer
                    <li>
                    learning rate = 5e-4
                    <li>
                    num_samples = 64
                    <li>
                    near = 0.05
                    <li>
                    far = 0.55
                    <li>
                    batch_size = 10000
                    <li>
                    num_iters = 9603
                    <li>
                    L_pos = 10
                    <li>
                    L_dir = 4
                    <li>
                    network width = 256
                    </ul>
                    <br>
                    For the number of training iterations, I allowed the model to keep training as long as I don't interrupt it.
                    In particular, I saved a checkpoint for the model once every 200 iterations, and additionally saved the rendered results of the first validation image every 500 iterations.
                    Otherwise, I did not make significant code changes.
                    <br><br>
                    This resulted in a model of the persimmon that is quite consistent across different views and that has a good amount of detail (especially at the leafs).
                    The ArUco tag itself was similarly reconstructed nicely.
                </p>

                <h3>Training Loss</h3>
                <p>
                    The training loss dropped nicely in this training configuration.
                    Interestingly, it briefly plateaus at around 200 iterations, then dropping steeply again, finally starting to plateau at around 1000 iterations.
                    I included a second loss curve with the first 9 iterations omitted to decrease the range of the losses and thereby make the rest of the curve more clear.
                    The validation PSNR grew nicely to around 24 within around 2000 iterations.
                </p>
                <div class="three-col-gallery">
                    <figure class="result-item">
                        <img src="./assets/2.6/training_losses.png" alt="Training Losses">
                        <figcaption>Training Losses</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/2.6/training_losses_with_omission.png" alt="Training Losses with Omission">
                        <figcaption>Training Losses (with omission)</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/2.6/val_psnrs.png" alt="Validation PSNRs">
                        <figcaption>Validation PSNRs</figcaption>
                    </figure>
                </div>

                <h3>Intermediate Renders</h3>
                <p>
                    I rendered results (for the first validation image) after certain iterations in training (Iteration i means result after the ith training iteration, 0-indexed).
                    The results got consistently better over the training iterations.
                    Interestingly, although the increase in PSNR and decrease in loss between the 2000th iteration and 9500th iteration were small, iteration 9500's results seem to reconstruct the lighting on the persimmon better.
                </p>
                <div class="three-col-gallery">
                    <figure class="result-item">
                        <img src="./assets/2.6/0.jpg" alt="Iteration 0">
                        <figcaption>Iteration 0</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/2.6/200.jpg" alt="Iteration 200">
                        <figcaption>Iteration 200</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/2.6/400.jpg" alt="Iteration 400">
                        <figcaption>Iteration 400</figcaption>
                    </figure>
                </div>
                <div class="three-col-gallery">
                    <figure class="result-item">
                        <img src="./assets/2.6/1000.jpg" alt="Iteration 1000">
                        <figcaption>Iteration 1000</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/2.6/2000.jpg" alt="Iteration 2000">
                        <figcaption>Iteration 2000</figcaption>
                    </figure>
                    <figure class="result-item">
                        <img src="./assets/2.6/9500.jpg" alt="Iteration 9500">
                        <figcaption>Iteration 9500</figcaption>
                    </figure>
                </div>

                <h3>Novel Views GIF</h3>
                <div class="small-single-image-container">
                    <figure class="result-item">
                        <img src="./assets/2.6/angle2.gif" alt="Novel Views">
                        <figcaption>Novel Views GIF</figcaption>
                    </figure>
                </div>
            </section>
        </main>

        <footer>
            <p>&copy; 2025 Tiger Zhang. <a href="mailto:enming_zhang@berkeley.edu">Contact Me</a></p>
        </footer>
    </div>
</body>
</html>
